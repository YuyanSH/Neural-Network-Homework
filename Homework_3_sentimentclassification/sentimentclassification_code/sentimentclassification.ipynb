{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63267d38",
   "metadata": {},
   "source": [
    "# 基于IMDB的情感分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1d3de",
   "metadata": {},
   "source": [
    "## 数据加载与预处理\n",
    "\n",
    "使用Keras 的内置 IMDB 数据集，将评论转换为固定长度的词索引序列，并构造 PyTorch 的 Dataset 和 DataLoader 以便后续训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de4e891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb23f2b",
   "metadata": {},
   "source": [
    "设置序列填充参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4450fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 词汇表大小（考虑常见的10000个词）\n",
    "maxlen = 200          # 文本序列最大长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78dcc3",
   "metadata": {},
   "source": [
    "加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e556db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size) # 加载IMDB数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc46e0a",
   "metadata": {},
   "source": [
    "将所有序列填充到固定长度，便于批量处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55721e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pad_sequences(train_data, maxlen=maxlen, padding='post') # 填充训练集序列为固定长度200，在序列末尾补0\n",
    "test_data = pad_sequences(test_data, maxlen=maxlen, padding='post') # 填充测试集序列为固定长度200，在序列末尾补0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748399b",
   "metadata": {},
   "source": [
    "构建pytorch数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d50678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data   \n",
    "        self.labels = labels  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels) # 获取数据集大小\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取单条样本的序列张量和标签张量\n",
    "        x = torch.tensor(self.data[idx], dtype=torch.long) # 使用 long 类型以配合嵌入层\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float)  # 使用 float 类型以适配二元交叉熵损失\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f767b4",
   "metadata": {},
   "source": [
    "实例化数据集以及设置批量处理加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ee5626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 25000, testing samples: 25000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MovieDataset(train_data, train_labels) # 将训练集数据封装为MovieDataset对象\n",
    "test_dataset  = MovieDataset(test_data, test_labels) # 将测试集数据封装为MovieDataset对象\n",
    "batch_size = 64 # 批大小\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # 读取一个批次的训练数据并且打乱\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # 读取一个批次的测试数据\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}, testing samples: {len(test_dataset)}\") # 打印训练集和测试集的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e03d1",
   "metadata": {},
   "source": [
    "## BERT模型\n",
    "使用BERT模型进行情感分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2256386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 设备选择（若有GPU则使用GPU）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e545fe8",
   "metadata": {},
   "source": [
    "将数字序列还原为英文单词序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a4e54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_index = imdb.get_word_index() # 获取词汇表映射\n",
    "reverse_word_index = {value+3: key for key, value in word_index.items()} # keras默认预留了前三个索引，因此需要将所有索引加3才能与单词对应\n",
    "reverse_word_index[0] = \"<PAD>\" # keras默认预留了0作为填充符\n",
    "reverse_word_index[1] = \"<START>\" # keras默认预留了1作为起始符\n",
    "reverse_word_index[2] = \"<UNK>\" # keras默认预留了2作为未知符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c049ba4",
   "metadata": {},
   "source": [
    "将序列还原为英文句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a192049",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = [] # 用于存储训练集文本\n",
    "for seq in train_data:\n",
    "    words = [reverse_word_index.get(idx, \"<PAD>\") for idx in seq] # 将索引转换为单词\n",
    "    words = [w for w in words if w not in [\"<PAD>\", \"<START>\", \"<UNK>\"]] # 去除填充符、起始符和未知符\n",
    "    texts_train.append(\" \".join(words)) # 将单词列表拼接为文本字符串\n",
    "\n",
    "texts_test = [] # 用于存储测试集文本\n",
    "for seq in test_data:\n",
    "    words = [reverse_word_index.get(idx, \"<PAD>\") for idx in seq] # 将索引转换为单词\n",
    "    words = [w for w in words if w not in [\"<PAD>\", \"<START>\", \"<UNK>\"]] # 去除填充符、起始符和未知符\n",
    "    texts_test.append(\" \".join(words)) # 将单词列表拼接为文本字符串"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300a4ef",
   "metadata": {},
   "source": [
    "使用BERT分词器对文本数据进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89774196",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # 加载一个预训练好的bert分词器\n",
    "max_len = 128 # 设置模型最大输入长度为128\n",
    "train_encodings = tokenizer(texts_train, truncation=True, padding=True, max_length=max_len) # 对训练集中所有文本进行bert编码\n",
    "test_encodings  = tokenizer(texts_test,  truncation=True, padding=True, max_length=max_len) # 对测试集中所有文本进行bert编码\n",
    "\n",
    "train_labels_list = train_labels.tolist() # 将标签数组转换为列表\n",
    "test_labels_list  = test_labels.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52979d4f",
   "metadata": {},
   "source": [
    "将bert编码后的数据转换为pytorch可处理的数据格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e4d4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings # bert编码后的数据字典\n",
    "        self.labels = labels # 标签列表\n",
    "    def __len__(self):\n",
    "        return len(self.labels) # 数据集长度\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} # 设置数据索引\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # 设置标签索引\n",
    "        return item\n",
    "\n",
    "train_dataset_bert = IMDBDataset(train_encodings, train_labels_list) # 创建bert编码后的训练集\n",
    "test_dataset_bert  = IMDBDataset(test_encodings,  test_labels_list) # 创建bert编码后的测试集\n",
    "train_loader_bert  = DataLoader(train_dataset_bert, batch_size=16, shuffle=True) # 创建bert编码后的训练集批次加载器\n",
    "test_loader_bert   = DataLoader(test_dataset_bert,  batch_size=16, shuffle=False) # 创建bert编码后的测试集批次加载器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfa8812",
   "metadata": {},
   "source": [
    "准备bert模型和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0d793d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device) # 加载预训练的bert模型\n",
    "optimizer = optim.Adam(model_bert.parameters(), lr=2e-5) # 定义优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c41e113",
   "metadata": {},
   "source": [
    "训练bert模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9859ae85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.0919\n",
      "Epoch 2/2, Loss: 0.0506\n"
     ]
    }
   ],
   "source": [
    "epochs = 2 # 训练轮次2轮\n",
    "for epoch in range(epochs):\n",
    "    model_bert.train() # 设置模型为训练模式\n",
    "    total_loss = 0.0 # 初始化总损失\n",
    "    for batch in train_loader_bert: # 对于每个批次的训练\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        input_ids = batch['input_ids'].to(device) # 将bert编码后的ID数据移到设备\n",
    "        attention_mask = batch['attention_mask'].to(device) # 将bert编码后的有效性参数移到设备\n",
    "        labels = batch['labels'].to(device) # 将bert编码后的标签数据移到设备\n",
    "        outputs = model_bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels) # 数据经过模型输出\n",
    "        loss = outputs.loss # 计算损失\n",
    "        loss.backward() # 反向传播计算梯度\n",
    "        optimizer.step() # 更新参数\n",
    "        total_loss += loss.item() # 累加损失\n",
    "    avg_loss = total_loss / len(train_loader_bert) # 平均损失\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\") # 打印对应轮次的损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ceb8df",
   "metadata": {},
   "source": [
    "在测试集上对模型进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4df99e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT模型测试准确率: 89.24%\n"
     ]
    }
   ],
   "source": [
    "model_bert.eval() # 将模型切换为评估模式\n",
    "correct = 0 # 初始化预测正确的样本数\n",
    "total = 0 # 初始化总样本数\n",
    "with torch.no_grad(): # 关闭梯度计算\n",
    "    for batch in test_loader_bert: # 对于测试集的每个批次\n",
    "        input_ids = batch['input_ids'].to(device)# 将当前批次的ID数据移到设备\n",
    "        attention_mask = batch['attention_mask'].to(device)# 将当前批次的有效性参数移到设备\n",
    "        labels = batch['labels'].to(device) # 将当前批次的标签数据移到设备\n",
    "        outputs = model_bert(input_ids=input_ids, attention_mask=attention_mask) # 将输入送入训练后的模型\n",
    "        logits = outputs.logits # 提取样本对不同类别的预测分数\n",
    "        preds = torch.argmax(logits, dim=1) # 找出最大值的索引\n",
    "        correct += (preds == labels).sum().item() # 将预测结果与真是标签比较\n",
    "        total += labels.size(0) # 样本总数\n",
    "accuracy = correct / total # 计算准确率\n",
    "print(f\"BERT模型测试准确率: {accuracy*100:.2f}%\") # 打印准确率\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
