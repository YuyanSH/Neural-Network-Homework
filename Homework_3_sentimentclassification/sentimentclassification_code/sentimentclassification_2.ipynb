{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63267d38",
   "metadata": {},
   "source": [
    "# 基于IMDB的情感分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1d3de",
   "metadata": {},
   "source": [
    "## 数据加载与预处理\n",
    "\n",
    "我们首先使用 TensorFlow Keras 的内置 IMDB 数据集（包含 25,000 条已标注的电影评论\n",
    "keras.io\n",
    "），将评论转换为固定长度的词索引序列，并构造 PyTorch 的 Dataset 和 DataLoader 以便后续训练。关键步骤包括：设置词汇表大小、最大序列长度、序列填充，以及将数据转换为 PyTorch 张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de4e891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4450fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 词汇表大小（只考虑最常见的10000个词）\n",
    "maxlen = 200          # 文本序列最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e556db63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading IMDB dataset...\")\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55721e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对序列进行填充，使其长度一致\n",
    "train_data = pad_sequences(train_data, maxlen=maxlen, padding='post')\n",
    "test_data = pad_sequences(test_data, maxlen=maxlen, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d50678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 PyTorch Dataset\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data    # 序列数据（numpy数组）\n",
    "        self.labels = labels  # 标签（0 或 1）\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 返回单条样本（序列张量、标签张量）\n",
    "        x = torch.tensor(self.data[idx], dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float)  # 使用 float 类型以配合 BCE 损失\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ee5626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 25000, testing samples: 25000\n"
     ]
    }
   ],
   "source": [
    "# 实例化 Dataset 和 DataLoader\n",
    "train_dataset = MovieDataset(train_data, train_labels)\n",
    "test_dataset  = MovieDataset(test_data, test_labels)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}, testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d50ca",
   "metadata": {},
   "source": [
    "## RNN模型\n",
    "\n",
    "我们首先实现一个**简单的RNN（循环神经网络）**模型，包括词嵌入层（Embedding）和一个基本的 RNN 层，以及一个全连接输出层。模型结构为：输入文本序列 → Embedding → RNN → 全连接层 → 输出单个标量。训练过程中使用二元交叉熵损失函数（BCEWithLogitsLoss）和 Adam 优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ec2edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.6885\n",
      "Epoch 2/3, Loss: 0.6704\n",
      "Epoch 3/3, Loss: 0.6481\n",
      "RNN 模型测试准确率: 55.09%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义 RNN 模型\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)        # 词嵌入层\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)  # 基础 RNN 层\n",
    "        self.fc = nn.Linear(hidden_dim, 1)                          # 输出层\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)              # [batch, seq_len] -> [batch, seq_len, embed_dim]\n",
    "        output, hidden = self.rnn(x)       # output: 所有时间步的输出, hidden: 最后时间步的隐藏状态\n",
    "        out = self.fc(hidden.squeeze(0))   # 取最后时间步的隐藏状态预测\n",
    "        return out\n",
    "\n",
    "# 设备选择（若有GPU则使用GPU）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 实例化模型、损失函数和优化器\n",
    "model_rnn = RNNModel(vocab_size=vocab_size, embed_dim=128, hidden_dim=128).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二元交叉熵（包含 Sigmoid）\n",
    "optimizer = optim.Adam(model_rnn.parameters(), lr=0.001)\n",
    "\n",
    "# 训练 RNN 模型\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model_rnn.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_rnn(inputs).squeeze(1)  # 模型输出形状 [batch, 1]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 在测试集上评估 RNN 模型\n",
    "model_rnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = torch.sigmoid(model_rnn(inputs).squeeze(1))  # 计算 Sigmoid 概率\n",
    "        preds = (outputs >= 0.5).float()  # 阈值分类\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "accuracy = correct / total\n",
    "print(f\"RNN 模型测试准确率: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b2479",
   "metadata": {},
   "source": [
    "## LSTM模型\n",
    "我们实现**LSTM（长短时记忆）**模型。与 RNN 类似，模型同样包含 Embedding 层和 LSTM 层，只是在循环层中替换为 nn.LSTM。LSTM 能更好地捕捉长程依赖，往往在文本任务上效果优于普通 RNN。其余训练设置与 RNN 相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748bdc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Loss: 0.6676\n",
      "Epoch 2/6, Loss: 0.5463\n",
      "Epoch 3/6, Loss: 0.4343\n",
      "Epoch 4/6, Loss: 0.4528\n",
      "Epoch 5/6, Loss: 0.4003\n",
      "Epoch 6/6, Loss: 0.3421\n",
      "LSTM 模型测试准确率: 79.87%\n"
     ]
    }
   ],
   "source": [
    "# 定义 LSTM 模型\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)           # 词嵌入层\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)   # LSTM 层\n",
    "        self.fc = nn.Linear(hidden_dim, 1)                             # 输出层\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                        # [batch, seq_len, embed_dim]\n",
    "        output, (hidden, cell) = self.lstm(x)        # hidden: [num_layers, batch, hidden_dim]\n",
    "        out = self.fc(hidden.squeeze(0))             # 取最后一层隐藏状态\n",
    "        return out\n",
    "\n",
    "# 实例化 LSTM 模型\n",
    "model_lstm = LSTMModel(vocab_size=vocab_size, embed_dim=128, hidden_dim=128).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "# 训练 LSTM 模型\n",
    "epochs = 6\n",
    "for epoch in range(epochs):\n",
    "    model_lstm.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_lstm(inputs).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 在测试集上评估 LSTM 模型\n",
    "model_lstm.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = torch.sigmoid(model_lstm(inputs).squeeze(1))\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "accuracy = correct / total\n",
    "print(f\"LSTM 模型测试准确率: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e03d1",
   "metadata": {},
   "source": [
    "## BERT模型\n",
    "最后，我们使用BERT模型（bert-base-uncased）进行情感分类。BERT 模型接收原始文本输入，因此需要先将索引序列还原为文本，然后使用 Hugging Face 的 BertTokenizer 和 BertForSequenceClassification。关键步骤如下：解码词索引序列为文本 → 使用 BERT 分词器编码文本 → 构建 PyTorch Dataset → 微调 BERT 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2256386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7157cff1cd04bcfba0dc77fd4e141d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\APP\\TechSoftware\\Anaconda\\anaconda3\\envs\\neural-network\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lenovo\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f12b7e7b1842fda5c876993b66b55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365d4b8deecc4dbe8e1b50b82a170c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f75d80c45424120a9f3e8892e2bcada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3476baa9215740bca2d74145c00612c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.3172\n",
      "Epoch 2/2, Loss: 0.1884\n",
      "BERT 模型测试准确率: 90.26%\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# 1. 将索引序列解码为原始文本\n",
    "word_index = imdb.get_word_index()  # Keras IMDB 的词到索引映射\n",
    "# 由于 Keras 在加载数据时使用了 index_from=3 的偏移，需要加回去\n",
    "reverse_word_index = {value+3: key for key, value in word_index.items()}\n",
    "reverse_word_index[0] = \"<PAD>\"\n",
    "reverse_word_index[1] = \"<START>\"\n",
    "reverse_word_index[2] = \"<UNK>\"\n",
    "\n",
    "# 生成文本列表（去除填充符号）\n",
    "texts_train = []\n",
    "for seq in train_data:\n",
    "    words = [reverse_word_index.get(idx, \"<PAD>\") for idx in seq]\n",
    "    # 去掉特殊标记\n",
    "    words = [w for w in words if w not in [\"<PAD>\", \"<START>\", \"<UNK>\"]]\n",
    "    texts_train.append(\" \".join(words))\n",
    "\n",
    "texts_test = []\n",
    "for seq in test_data:\n",
    "    words = [reverse_word_index.get(idx, \"<PAD>\") for idx in seq]\n",
    "    words = [w for w in words if w not in [\"<PAD>\", \"<START>\", \"<UNK>\"]]\n",
    "    texts_test.append(\" \".join(words))\n",
    "\n",
    "# 2. 使用 BERT 分词器对文本进行编码\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 128  # BERT 最大序列长度（可根据显存调整）\n",
    "train_encodings = tokenizer(texts_train, truncation=True, padding=True, max_length=max_len)\n",
    "test_encodings  = tokenizer(texts_test,  truncation=True, padding=True, max_length=max_len)\n",
    "\n",
    "# 将标签转换为列表（长整数）\n",
    "train_labels_list = train_labels.tolist()\n",
    "test_labels_list  = test_labels.tolist()\n",
    "\n",
    "# 3. 构建 PyTorch Dataset\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset_bert = IMDBDataset(train_encodings, train_labels_list)\n",
    "test_dataset_bert  = IMDBDataset(test_encodings,  test_labels_list)\n",
    "train_loader_bert  = DataLoader(train_dataset_bert, batch_size=16, shuffle=True)\n",
    "test_loader_bert   = DataLoader(test_dataset_bert,  batch_size=16, shuffle=False)\n",
    "\n",
    "# 4. 加载预训练 BERT 模型（2 分类）\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "izer = optim.Adam(model_bert.parameters(), lr=2e-5)\n",
    "\n",
    "# 5. 训练 BERT 模型（微调）\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    model_bert.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader_bert:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader_bert)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 6. 在测试集上评估 BERT 模型\n",
    "model_bert.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_bert:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "accuracy = correct / total\n",
    "print(f\"BERT 模型测试准确率: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e54ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
